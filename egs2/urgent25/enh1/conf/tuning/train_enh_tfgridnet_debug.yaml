use_amp: false
optim: adam
init: none
unused_parameters: false
max_epoch: 100
batch_type: folded
batch_size: 2 # batch_size 2 can be trained on 2 A5000 GPU
iterator_type: chunk
chunk_length: 200 # 4s
chunk_default_fs: 50 # GCD among all possible sampling frequencies
num_cache_chunks: 16
num_iters_per_epoch: 8000
num_workers: 4
grad_clip: 1.0
optim_conf:
    lr: 2.5e-04
    eps: 1.0e-08
    weight_decay: 1.0e-05
patience: 40
val_scheduler_criterion:
- valid
- loss
best_model_criterion:
-   - valid
    - loss
    - min
keep_nbest_models: 1
scheduler: warmupsteplr
scheduler_conf:
    step_size: 2
    gamma: 0.99
    warmup_steps: 4000

allow_multi_rates: true

preprocessor: enh
preprocessor_conf:
    speech_volume_normalize: "0.5_1.0"
    rir_scp: data/rir_train.scp
    rir_apply_prob: 0.5
    noise_scp: data/noise_train.scp
    noise_apply_prob: 1.0
    noise_db_range: "-5_15"
    force_single_channel: true
    channel_reordering: true
    # The categories list order must be the same everywhere in this config
    categories: &categories
    - 1ch_8000Hz
    - 1ch_16000Hz
    - 1ch_22050Hz
    - 1ch_24000Hz
    - 1ch_32000Hz
    - 1ch_44100Hz
    - 1ch_48000Hz
    - 1ch_8000Hz_reverb
    - 1ch_16000Hz_reverb
    - 1ch_22050Hz_reverb
    - 1ch_24000Hz_reverb
    - 1ch_32000Hz_reverb
    - 1ch_44100Hz_reverb
    - 1ch_48000Hz_reverb
    data_aug_effects:   # no need to set the "sample_rate" argument for each effect here
    - [1.0, "bandwidth_limitation", {"res_type": "random"}]
    - [1.0, "clipping", {"min_quantile": 0.1, "max_quantile": 0.9}]
    - [1.0, [[0.5, "codec", {"format": "mp3", "encoder": null, "qscale": [1, 10]}], [0.5, "codec", {"format": "ogg", "encoder": ["vorbis", "opus"], "qscale": [-1, 10]}]]]
    # - [1.0, "codec", [{"format": "mp3", "encoder": null, "qscale": [1, 10]}, {"format": "ogg", "encoder": ["vorbis", "opus"], "qscale": [-1, 10]}]]
    - [1.0, "packet_loss", {"packet_duration_ms": 20, "packet_loss_rate": [0.05, 0.25], "max_continuous_packet_loss": 10}]
    # - [1.0, "mp3_codec", {"vbr_quality": [0.0, 10.0]}]
    data_aug_num: [1, 3]  # at most 1 augmentation per sample
    data_aug_prob: 0.75
num_spk: 1

model_conf:
    normalize_variance_per_ch: true
    #always_forward_in_48k: true
    # The categories list order must be the same everywhere in this config
    categories: *categories

encoder: stft
encoder_conf:
    n_fft: 256
    hop_length: 128
    use_builtin_complex: true
    default_fs: 8000
decoder: stft
decoder_conf:
    n_fft: 256
    hop_length: 128
    default_fs: 8000
separator: tfgridnetv3
separator_conf:
    n_srcs: 1
    n_imics: 1
    n_layers: 1
    lstm_hidden_units: 16
    attn_n_head: 4
    attn_qk_output_channel: 2
    emb_dim: 16
    emb_ks: 1
    emb_hs: 1
    activation: prelu
    eps: 1.0e-05

# A list for criterions
# The overlall loss in the multi-task learning will be:
# loss = weight_1 * loss_1 + ... + weight_N * loss_N
# The default `weight` for each sub-loss is 1.0
criterions:
  # The first criterion
  - name: mr_l1_tfd
    conf:
      window_sz: [256]
      hop_sz: null
      eps: 1.0e-8
      time_domain_weight: 0.5
      normalize_variance: true
    wrapper: fixed_order
    wrapper_conf:
      weight: 1.0
  # The second criterion
  - name: si_snr
    conf:
      eps: 1.0e-7
    wrapper: fixed_order
    wrapper_conf:
      weight: 0.0

use_wandb: false
wandb_project: urgent2025
wandb_entity: kohei0209
wandb_name: tfgridnet_debug