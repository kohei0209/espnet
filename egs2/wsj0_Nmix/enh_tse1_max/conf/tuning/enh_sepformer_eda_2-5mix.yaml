optim: adam
init: none
max_epoch: 200
batch_type: folded
batch_size: 4 # batch_size 4 can be trained on 4 RTX 2080ti GPUs
valid_batch_size: 1
iterator_type: sequence
chunk_length: 32000
# num_iters_valid: 5000
# exclude keys "enroll_ref", "enroll_ref1", "enroll_ref2", ...
# from the length consistency check in ChunkIterFactory
chunk_excluded_key_prefixes:
- "enroll_ref"
num_workers: 4
unused_parameters: true
grad_clip: 5
optim_conf:
    lr: 1.5e-04
    eps: 1.0e-08
    weight_decay: 1.0e-05
patience: 10
val_scheduler_criterion:
- valid
- loss
best_model_criterion:
-   - valid
    - si_snr
    - max
-   - valid
    - loss
    - min
keep_nbest_models: 1
scheduler: reducelronplateau
scheduler_conf:
   mode: min
   factor: 0.5
   patience: 3

model_conf:
  num_spk: 5
  share_encoder: true
  task: enh

train_spk2enroll: data/tr_min_8k/spk2enroll.json
enroll_segment: 32000
load_spk_embedding: false
load_all_speakers: true

encoder: conv
encoder_conf:
    channel: 256
    kernel_size: 16
    stride: 8
decoder: conv
decoder_conf:
    channel: 256
    kernel_size: 16
    stride: 8
extractor: sepformer_eda
extractor_conf:
    num_spk: 5
    intra_layer_before_eda: 4
    inter_layer_before_eda: 2
    intra_layer_after_eda: 2
    inter_layer_after_eda: 1
    channel_layer: 0
    unit: 2048
    att_heads: 8
    dropout: 0.0
    activation: relu
    norm_type: gLN
    segment_size: 250
    nonlinear: relu

# A list for criterions
# The overlall loss in the multi-task learning will be:
# loss = weight_1 * loss_1 + ... + weight_N * loss_N
# The default `weight` for each sub-loss is 1.0
criterions:
  # The first criterion
  - name: si_snr
    conf:
      eps: 1.0e-7
    wrapper: pit
    wrapper_conf:
      weight: 1.0
      independent_perm: True


use_wandb: true
wandb_project: use_preliminary_wsj
wandb_name: Sepformer_Sep_EDA_wo_ChannelAttn