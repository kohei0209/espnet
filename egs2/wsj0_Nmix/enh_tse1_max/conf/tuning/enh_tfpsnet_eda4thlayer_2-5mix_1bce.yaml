optim: adam
max_epoch: 50
batch_type: folded
sort_batch: descending
batch_size: 1
valid_batch_size: 1
iterator_type: sequence
chunk_length: 32000
# exclude keys "enroll_ref", "enroll_ref1", "enroll_ref2", ...
# from the length consistency check in ChunkIterFactory
chunk_excluded_key_prefixes:
- "enroll_ref"
- "utt2category"
# - "speech_ref"
num_workers: 8
optim_conf:
    lr: 4.0e-04
    eps: 1.0e-04
    weight_decay: 0
unused_parameters: true
patience: 5
accum_grad: 1
grad_clip: 5.0
val_scheduler_criterion:
- valid
- loss
best_model_criterion:
-   - valid
    - snr
    - max
-   - valid
    - loss
    - min
keep_nbest_models: 10
scheduler: warmupreducelr
scheduler_conf:
   # for WarmupLR
   warmup_steps: 10000
   factor: 0.7
   patience: 1

model_conf:
    num_spk: 5
    share_encoder: true
    task: enh
    normalization: true

train_spk2enroll: data/tr_min_8k/spk2enroll.json
enroll_segment: 32000
load_spk_embedding: false
load_all_speakers: true

encoder: stft
encoder_conf:
    n_fft: 256
    hop_length: 128
decoder: stft
decoder_conf:
    n_fft: 256
    hop_length: 128
extractor: tfpsnet_eda
extractor_conf:
    num_spk: 1
    enc_channels: 128
    bottleneck_size: 32
    separator_type: transformer
    tfps_blocks: [1, 1, 1, 1, 1, 1]
    rnn_type: lstm
    bidirectional: true
    unit: 128
    dropout: 0.0
    norm_type: gLN
    nonlinear: relu
    i_eda_layer: 3
    i_adapt_layer: null
    adapt_enroll_dim: 64

# A list for criterions
# The overlall loss in the multi-task learning will be:
# loss = weight_1 * loss_1 + ... + weight_N * loss_N
# The default `weight` for each sub-loss is 1.0
criterions:
  # The first criterion
  - name: si_snr
    conf:
      eps: 1.0e-8
    wrapper: pit
    wrapper_conf:
      weight: 1
      independent_perm: true

use_amp: false
use_wandb: true
wandb_project: use_preliminary_wsj
wandb_name: SmallTFPSNet_Sep_EDA4thLayer_2-5mix_SISNRLoss+1BCELoss_ReduceLR_lr4e-4