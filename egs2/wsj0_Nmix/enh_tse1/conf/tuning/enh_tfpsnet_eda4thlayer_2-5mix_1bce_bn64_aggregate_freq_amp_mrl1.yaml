optim: adam
max_epoch: 50
batch_type: folded
sort_batch: descending
batch_size: 1
valid_batch_size: 1
iterator_type: sequence
chunk_length: 32000
# exclude keys "enroll_ref", "enroll_ref1", "enroll_ref2", ...
# from the length consistency check in ChunkIterFactory
chunk_excluded_key_prefixes:
  - "enroll_ref"
  - "utt2category"

num_workers: 8
optim_conf:
  lr: 2.0e-04
  eps: 1.0e-04
  weight_decay: 0
unused_parameters: true
patience: 5
accum_grad: 1
grad_clip: 5.0
val_scheduler_criterion:
  - valid
  - loss
best_model_criterion:
  - - valid
    - snr
    - max
  - - valid
    - loss
    - min
keep_nbest_models: 10
scheduler: warmupreducelr
scheduler_conf:
  # for WarmupLR
  warmup_steps: 10000
  factor: 0.7
  patience: 1

model_conf:
  num_spk: 5
  share_encoder: true
  task: enh
  normalization: true

train_spk2enroll: data/tr_min_8k/spk2enroll.json
enroll_segment: 32000
load_spk_embedding: false
load_all_speakers: true

encoder: stft
encoder_conf:
  n_fft: 256
  hop_length: 128
decoder: stft
decoder_conf:
  n_fft: 256
  hop_length: 128
extractor: tfpsnet_eda
extractor_conf:
  num_spk: 1
  enc_channels: 256
  bottleneck_size: 64
  masking: true
  separator_type: transformer
  tfps_blocks: [1, 1, 1, 1, 1, 1]
  rnn_type: lstm
  bidirectional: true
  unit: 128
  dropout: 0.0
  norm_type: gLN
  nonlinear: relu
  i_eda_layer: 3
  i_adapt_layer: null
  adapt_enroll_dim: 64

# A list for criterions
# The overlall loss in the multi-task learning will be:
# loss = weight_1 * loss_1 + ... + weight_N * loss_N
# The default `weight` for each sub-loss is 1.0
criterions:
  # The first criterion
  - name: mr_l1_tfd
    conf:
      eps: 1.0e-4
      scale_invariant: false
      mean_or_sum: sum
    wrapper: pit
    wrapper_conf:
      weight: 0.01
      independent_perm: true
  - name: si_snr
    conf:
      eps: 1.0e-4
    wrapper: pit
    wrapper_conf:
      weight: 0
      independent_perm: false

use_amp: true
use_wandb: true
# log_interval: 100
wandb_project: muse_jornal
wandb_name: Anechoic_Sep_TFPSNet_2-5mix_MRL1+1BCE_ReduceLR_lr2e-4_AMP
