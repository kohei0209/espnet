task: &task tse
optim: adam
max_epoch: 50
batch_type: folded
sort_batch: descending
batch_size: 4
valid_batch_size: 4
iterator_type: sequence
chunk_length: 32000
# exclude keys "enroll_ref", "enroll_ref1", "enroll_ref2", ...
# from the length consistency check in ChunkIterFactory
chunk_excluded_key_prefixes:
  - "enroll_ref"
  - "utt2category"
# - "speech_ref"
n_mix:
  - 2
num_workers: 8
optim_conf:
  lr: 1.0e-04
  eps: 1.0e-08
  weight_decay: 0
unused_parameters: true
patience: 10
accum_grad: 1
grad_clip: 5.0
val_scheduler_criterion:
  - valid
  - loss
best_model_criterion:
  - - valid
    - snr
    - max
  - - valid
    - loss
    - min
keep_nbest_models: 5
scheduler: warmupsteplr
scheduler_conf:
  # for WarmupLR
  warmup_steps: 10000
  # for StepLR
  steps_per_epoch: 20120
  step_size: 2
  gamma: 0.98

model_conf:
  num_spk: 5
  share_encoder: true
  task: *task
  normalization: true

train_spk2enroll: data/tr_min_8k/spk2enroll.json
enroll_segment: 32000
load_spk_embedding: false
load_all_speakers: false

encoder: conv
encoder_conf:
  channel: 64
  kernel_size: 16
  stride: 8
decoder: conv
decoder_conf:
  channel: 64
  kernel_size: 16
  stride: 8
extractor: dptnet_eda
extractor_conf:
  num_spk: 5
  layer: 6
  rnn_type: lstm
  bidirectional: True # this is for the inter-block rnn
  unit: 128
  segment_size: 100
  att_heads: 4
  dropout: 0
  activation: relu
  norm_type: gLN
  nonlinear: prelu
  triple_path: false
  i_eda_layer: 3
  i_adapt_layer: 4
  adapt_layer_type: attn_improved
  adapt_enroll_dim: 64
  adapt_attention_dim: 512
  adapt_hidden_dim: 512
  adapt_softmax_temp: 1

# A list for criterions
# The overlall loss in the multi-task learning will be:
# loss = weight_1 * loss_1 + ... + weight_N * loss_N
# The default `weight` for each sub-loss is 1.0
criterions:
  # The first criterion
  - name: si_snr
    conf:
      eps: 1.0e-7
    wrapper: pit
    wrapper_conf:
      weight: 1.0
      independent_perm: true

use_wandb: true
wandb_project: use_preliminary_wsj
wandb_name: DPTNet_TSE_EDA4thLayer_ImprovedAdapt5thlayer_2mix

init_param:
  - ./exp/enh_enh_dptnet_eda4thlayer_2mix_1bce_copy_until175epochs/ave_epochs_166_170_173_175_161.pth
resume: false
freeze_param:
  - encoder
  - extractor.enc_LN
  - extractor.dptnet.row_transformer
  - extractor.dptnet.col_transformer
  - extractor.dptnet.output
  - extractor.dptnet.sequence_aggregation
  - extractor.dptnet.eda
  - extractor.output
  - extractor.nonlinear
  - extractor.output_gate
  - decoder
