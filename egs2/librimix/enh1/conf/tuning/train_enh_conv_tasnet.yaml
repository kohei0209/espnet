optim: adam
init: xavier_uniform
max_epoch: 200
batch_type: folded
batch_size: 16 # batch_size 16 can be trained on 4 RTX 2080ti
iterator_type: chunk
chunk_length: 24000
num_workers: 4
optim_conf:
    lr: 1.0e-03
    weight_decay: 0
patience: 5
grad_clip: 5.0
val_scheduler_criterion:
- valid
- loss
best_model_criterion:
-   - valid
    - si_snr
    - max
-   - valid
    - loss
    - min
keep_nbest_models: 1
scheduler: reducelronplateau
scheduler_conf:
    mode: min
    factor: 0.5
    patience: 1
model_conf:
    loss_type: si_snr
encoder: conv
encoder_conf:
    channel: 512
    kernel_size: 16
    stride: 8
decoder: conv
decoder_conf:
    channel: 512
    kernel_size: 16
    stride: 8
separator: tcn
separator_conf:
    num_spk: 2
    layer: 8
    stack: 3
    bottleneck_dim: 128
    hidden_dim: 512
    kernel: 3
    causal: False
    norm_type: "gLN"
    nonlinear: relu

# A list for criterions
# The overlall loss in the multi-task learning will be:
# loss = weight_1 * loss_1 + ... + weight_N * loss_N
# The default `weight` for each sub-loss is 1.0
criterions:
#   # The first criterion
   - name: si_snr
     conf:
       eps: 1.0e-7
     wrapper: pit
     wrapper_conf:
       weight: 1.0

use_wandb: true
wandb_project: universal_speech_enhancement
wandb_name: convtasnet_librimix_sep_bs4
