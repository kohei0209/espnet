optim: adam
init: xavier_uniform
max_epoch: 100
batch_type: folded
batch_size:  32
iterator_type: chunk
chunk_length: 32000
num_workers: 8
optim_conf:
    lr: 2.0e-04
    eps: 1.0e-08
    weight_decay: 0
patience: 10
accum_grad: 1
grad_clip: 5.0
val_scheduler_criterion:
- valid
- loss
best_model_criterion:
-   - valid
    - si_snr
    - max
-   - valid
    - loss
    - min
keep_nbest_models: 1
scheduler: reducelronplateau
scheduler_conf:
    mode: min
    factor: 0.7
    patience: 3

encoder: stft
encoder_conf:
    n_fft: 512
    hop_length: 128
decoder: stft
decoder_conf:
    n_fft: 512
    hop_length: 128
separator: asenet
separator_conf:
    num_spk: 2
    separator_hidden_dim: 512
    separator_num_layers: 2
    maskestimator_hidden_dim: 512
    maskestimator_num_layers: 2
    attention_dim: 200
    dropout_p: 0.3

# A list for criterions
# The overlall loss in the multi-task learning will be:
# loss = weight_1 * loss_1 + ... + weight_N * loss_N
# The default `weight` for each sub-loss is 1.0
criterions:
  # The first criterion
  - name: si_snr
    conf:
      eps: 1.0e-7
    wrapper: pit
    wrapper_conf:
      weight: 1.0
      independent_perm: True

use_wandb: true
wandb_project: universal_speech_enhancement
wandb_name: asenet_large_sep_trial_bs32